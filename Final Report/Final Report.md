---
title: Metwork robustness and percolation theory
author: Spina Lorenzo, Piermarco , Trapanotto Martino
date: October 27, 2022
---
# Introduction

In this work, we want to try and tackle the topic of network robustness, both for random networks and for more realistic network topologies.

With the term robustness, we mean the capacity of a network to keep it's connected nature under node failure, both randomized and targeted. 
This should be a reasonable way to approach the idea of the resilience of a network to both random failure of components and targeted attacks, and allow us to lay the groundwork for more advanced studies, such as enhancing robustness, recognizing more vulnerable or higher priority targets and more.

To analyze the topic, we'll use the conceptual and mathematical framework defined by Percolation Theory, a branch of mathematics that studies the evolution on networks when nodes and edges are added.

# Percolation Theory

The origin of the name stems from the question posed in the 1957 paper that originated the filed: If liquid is poured of a porous material, does the liquid flow through the cavities until reaching the bottom?

Percolation theory models this phenomenon as a 3D lattice structure where nodes represent holes in the material and links possible passages between these cavities. Thus, creating a statistical model for the porous material.

The filed has expanded as a branch of mathematics and statistical physics and is quite richly explored, with applications now ranging in ecology, biology, biochemistry, virology, statistical modeling and, as we'll show, network science.

## Lattices and networks

Lattices are the most common model used in percolation theory, constructing a regular network where nodes are connected only to their relative neighbors in a regular pattern. These can be square, triable, hexagon, 2D, 3D, 4D.

All these share some common properties, given by the number of dimensions. The parameters that define these properties are called *Critical Exponents*.

These structures are modelled by placing the nodes in our lattice, and having the edges be generated by a random variable. Each edge has the same probability $p$ of existing, so using a Bernoulli model.

One of the key findings of percolation theory is that the distribution connecting $p$ and properties of the generated graph, such as average cluster size or likelihood of reachability between nodes, are not linked linearly. 
Instead, there is a key value $p_c$ called *Critical probability*, and when $p>p_c$ the network structure changes: The smaller cluster connect and a major network component, usually called *percolating cluster*, appears and connects the majority of the network. 

This process can be looked in reverse, starting from a fully formed and connected network, and removing nodes with a small probability $f$, until the major connected component fails, and the network is completely shattered.

As for the creation process, this *Inverse Percolation* is not a smooth process but an abrupt one, where the network can reroute connections and paths until a value $f_c = 1-p_c$ called *Critical Threshold*, is reached and the network collapses.

All these processes and the values governing them can be computed for the aforementioned lattices, but have also been studied and analyzed for other topologies, such as random networks or Scale-Free models. This allows Percolation theory to model failures over a wide range of network types. 

# Random networks

The first network we model is an Erdős–Rényi random graph. This model has a given number of nodes and edges are connected randomly.

Random networks can be analyzed directly under percolation theory as infinite-dimension lattices, and have as such well defined critical exponents.



## Impact of random failures

# Scale-Free Networks

## Impact of random failures

# Real world networks

# Targeted failures

## Impacts on random networks

## Impacts on scale-free networks

## Impacts on real world networks

